{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rypotter/rag_from_scratch/blob/main/MultilangEmb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHFlkkCOEgBV"
      },
      "source": [
        "# OpenAI vs open-source multilingual embeddings models\n",
        "\n",
        "This noteboook provides example code to assess which embedding model works best for your data. The example task is a retrieval task (as in RAG - retrieval augmented generation), on multilingual data. See associated Medium article [here](https://medium.com/p/e5ccb7c90f05).\n",
        "\n",
        "The data source is based on the European AI Act, and models cover some of the latest OpenAI and open-source embeddings models (as of 02/2024) to deal with multilingual data:\n",
        "\n",
        "OpenAI released [two models](https://openai.com/blog/new-embedding-models-and-api-updates) in January 2024:\n",
        "\n",
        "- text-embedding-3-small (released 25/01/2024)\n",
        "- text-embedding-3-large (released 25/01/2024)\n",
        "\n",
        "We compare with the following open-source models\n",
        "\n",
        "- [e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct) (released 04/01/2024)\n",
        "- [multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct) (released 08/02/2024)\n",
        "- [BGE-M3](https://huggingface.co/BAAI/bge-m3) (released 29/01/2024)\n",
        "- [nomic-embed-text-v1](https://huggingface.co/nomic-ai/nomic-embed-text-v1) (released 10/02/2024)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK0YmoAlm4BE",
        "outputId": "b3072aa3-5972-423a-8ac9-91c47bcb9c57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 820\n",
            "drwxr-xr-x 1 root root   4096 Nov  9 15:46 .\n",
            "drwxr-xr-x 1 root root   4096 Nov  9 15:33 ..\n",
            "drwxr-xr-x 4 root root   4096 Nov  7 20:56 .config\n",
            "-rw-r--r-- 1 root root 822598 Nov  9 15:46 HU_dataset.json\n",
            "drwxr-xr-x 1 root root   4096 Nov  7 20:56 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HWmbfN8EgBX"
      },
      "source": [
        "# Install and import libraries\n",
        "\n",
        "LlamaIndex will be used for loading embedding models and documents, and evaluate retrieval accuracy.\n",
        "\n",
        "Seaborn will be used for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiIlNpJ3KXM4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -q llama_index==0.10.1\n",
        "!pip install -q html2text #Required for loading web pages with Llama index's SimpleWebPageReader\n",
        "!pip install -q llama-index-readers-web #Required for loading web pages with Llama index's SimpleWebPageReader\n",
        "!pip install -q transformers==4.37.2 # Required for open-source embeddings\n",
        "!pip install -q openai # Required for GPT and OpenAI embeddings\n",
        "!pip install -q einops #Required for nomic\n",
        "!pip install -q seaborn #Required for visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRa_JuD--yU8"
      },
      "source": [
        "Let us load the libraries/functions required for running this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HgrTbzREl0y"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "079gEGjx_GWt"
      },
      "source": [
        "Put your OpenAI API key here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12qjBEgB_HKS"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"sk-...\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6LE69rcEgBZ"
      },
      "source": [
        "# Generate a multilingual Question/Answer dataset\n",
        "\n",
        "We'll use the EU AI Act as the corpus. The text in different languages is available at All multilingual versions at https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206.\n",
        "\n",
        "Available languages: \"BG\", \"ES\", \"CS\", \"DA\", \"DE\", \"ET\", \"EL\", \"EN\", \"FR\", \"GA\", \"HR\", \"IT\", \"LV\", \"LT\", \"HU\", \"MT\", \"NL\", \"PL\", \"PT\", \"RO\", \"SK\", \"SL\", \"FI\", \"SV\".\n",
        "\n",
        "We'll use\n",
        "\n",
        "- EN: English (Germanic)\n",
        "- FR: French (Romance)\n",
        "- CS: Czech (Slavic)\n",
        "- HU: Hungarian (Uralic)\n",
        "\n",
        "\n",
        "Useful documentation from Llama Index:\n",
        "- https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo.html\n",
        "- https://docs.llamaindex.ai/en/latest/examples/finetuning/embeddings/finetune_embedding.html#\n",
        "- https://docs.llamaindex.ai/en/stable/api/llama_index.node_parser.SentenceSplitter.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "9c4ce4fbf6db49ac8cd1c92dd691f18a",
            "5aea427f122c490990c6d3e86b971236",
            "affb1620da1f4613a07541be3c58fc88",
            "79e705f2caf540a586a4e41e7f091656",
            "519cd748fc7347b68c86dc9e6b00bc49",
            "8a1d6f9bc3ed44c58ba9871b679c570c",
            "b592ecdb59094a0e998a76e302047770",
            "1359923c9b314fca92cf6ec84edd44b5",
            "97ce0a57af614a47b11fbfb9965a1b48",
            "6dcfc94deb394da5af5a32870e5afd22",
            "024f3018c5b049138bda5ff9331ff9fd",
            "c590fb747bbb4f72882ce55fae267dab"
          ]
        },
        "id": "3It-2gNTJAQ8",
        "outputId": "b0b72a74-1c3f-4aff-d674-323f330d5661"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c590fb747bbb4f72882ce55fae267dab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "language = \"EN\"\n",
        "url_doc = \"https://eur-lex.europa.eu/legal-content/\"+language+\"/TXT/HTML/?uri=CELEX:52021PC0206\"\n",
        "\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data([url_doc])\n",
        "\n",
        "parser = SentenceSplitter(chunk_size=1000)\n",
        "nodes = parser.get_nodes_from_documents(documents, show_progress=True)\n",
        "\n",
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsw68DgkD0S7"
      },
      "outputs": [],
      "source": [
        "# Example of text for the first chunck\n",
        "nodes[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2knTuTKGOayo"
      },
      "source": [
        "### Prompts for generating questions and answers\n",
        "\n",
        "We use the default prompt at: https://github.com/run-llama/llama_index/blob/c058f2531ea86ee74822cb1421ceaeee7098a99f/llama_index/finetuning/embeddings/common.py#L51\n",
        "\n",
        "The French, Czech and Hungarian prompt were obtained using DeepL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPk3Fi62LA_3"
      },
      "outputs": [],
      "source": [
        "prompts={}\n",
        "\n",
        "prompts[\"EN\"] = \"\"\"\\\n",
        "Context information is below.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Given the context information and not prior knowledge, generate only questions based on the below query.\n",
        "\n",
        "You are a Teacher/ Professor. Your task is to setup {num_questions_per_chunk} questions for an upcoming quiz/examination.\n",
        "The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\n",
        "\"\"\"\n",
        "\n",
        "prompts[\"FR\"] = \"\"\"\\\n",
        "Les informations contextuelles se trouvent ci-dessous.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Compte tenu des informations contextuelles et sans connaissances préalables, générer uniquement des questions basées sur la requête ci-dessous.\n",
        "\n",
        "Vous êtes enseignant/professeur. Votre tâche consiste à mettre en place {num_questions_per_chunk} question(s) par requête pour un quiz à venir.\n",
        "Les questions doivent être de nature variée sur l'ensemble du document. Limitez les questions aux informations contextuelles fournies.\n",
        "Les questions doivent être en français.\n",
        "\"\"\"\n",
        "\n",
        "prompts[\"HU\"] = \"\"\"\\\n",
        "A kontextusra vonatkozó információk alább találhatók.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "A kontextusinformáció és az előzetes ismeretek nélkül csak az alábbi lekérdezésen alapuló kérdéseket generáljon.\n",
        "\n",
        "Ön tanár/professzor. Az Ön feladata, hogy {num_questions_per_chunk} kérdéseket állítson össze egy közelgő kvízhez/vizsgához.\n",
        "A kérdéseknek változatos jellegűeknek kell lenniük az egész dokumentumban. Korlátozza a kérdéseket a megadott kontextus információkra.\n",
        "A kérdéseknek magyarul kell megszólalniuk.\n",
        "\"\"\"\n",
        "\n",
        "prompts[\"CS\"] = \"\"\"\\\n",
        "Související informace naleznete níže.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Vzhledem ke kontextovým informacím a bez jakýchkoli předchozích znalostí vygenerujte pouze otázky na základě níže uvedeného dotazu.\n",
        "\n",
        "Jste učitel. Vaším úkolem je sestavit {num_questions_per_chunk} otázek na jeden dotaz pro nadcházející kvíz.\n",
        "Otázky musí být v celém dokumentu různorodé. Omezte otázky na uvedené kontextové informace.\n",
        "Otázky by měly být v češtině.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6s8D7KOtfn"
      },
      "source": [
        "### Synthetic question/answer generation using OpenAI GPT3.5\n",
        "\n",
        "We use gpt-3.5-turbo-0125, which is according to OpenAI the flagship model of this family, supports a 16K context window and is optimized for dialog (https://platform.openai.com/docs/models/gpt-3-5-turbo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnJU9xWoKP1C",
        "outputId": "85b7ca40-03c6-4c02-9d15-d8c0d0501da0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 36/190 [02:58<04:34,  1.78s/it]"
          ]
        }
      ],
      "source": [
        "#from llama_index.llms import OpenAI\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.legacy.finetuning import generate_qa_embedding_pairs\n",
        "\n",
        "qa_dataset = generate_qa_embedding_pairs(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-0125\",additional_kwargs={'seed':42}),\n",
        "    nodes=nodes,\n",
        "    qa_generate_prompt_tmpl = prompts[language],\n",
        "    num_questions_per_chunk=2\n",
        ")\n",
        "\n",
        "file_name=language+\"_dataset.json\"\n",
        "qa_dataset.save_json(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjzW-McSl8jU"
      },
      "source": [
        "### Load dataset from JSON file\n",
        "\n",
        "Data can be reloaded from their JSON file with the EmbeddingQAFinetuneDataset object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDhv6wP7LpIg"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
        "\n",
        "language = \"EN\"\n",
        "file_name=language+\"_dataset.json\"\n",
        "qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HJGaF5ASz5X",
        "outputId": "ed77a7d9-0f34-41cf-98b9-51c07095d46e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What are the main objectives of the proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) according to the explanatory memorandum?',\n",
              " 'How does the proposal for a Regulation on artificial intelligence aim to address the risks associated with the use of AI while promoting the uptake of AI in the European Union, as outlined in the context information?']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(qa_dataset.queries.values())[0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP5FtNufEgBb"
      },
      "source": [
        "# Evaluate OpenAI embedding models\n",
        "\n",
        "The code inspired from https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html\n",
        "\n",
        "The function `evaluate` assesses the performance of a text embedding model on a given dataset for information retrieval tasks. Assessment is in terms of MRR (Mean Reciprocal Rank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ssne7NyJ6a1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "def evaluate(dataset, embed_model, insert_batch_size=1000, top_k=5):\n",
        "    # Get corpus, queries, and relevant documents from the qa_dataset object\n",
        "    corpus = dataset.corpus\n",
        "    queries = dataset.queries\n",
        "    relevant_docs = dataset.relevant_docs\n",
        "\n",
        "    # Create TextNode objects for each document in the corpus and create a VectorStoreIndex to efficiently store and retrieve embeddings\n",
        "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
        "    index = VectorStoreIndex(\n",
        "        nodes, embed_model=embed_model, insert_batch_size=insert_batch_size\n",
        "    )\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    # Prepare to collect evaluation results\n",
        "    eval_results = []\n",
        "\n",
        "    # Iterate over each query in the dataset to evaluate retrieval performance\n",
        "    for query_id, query in tqdm(queries.items()):\n",
        "        # Retrieve the top_k most similar documents for the current query and extract the IDs of the retrieved documents\n",
        "        retrieved_nodes = retriever.retrieve(query)\n",
        "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
        "\n",
        "        # Check if the expected document was among the retrieved documents\n",
        "        expected_id = relevant_docs[query_id][0]\n",
        "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc per query\n",
        "\n",
        "        # Calculate the Mean Reciprocal Rank (MRR) and append to results\n",
        "        if is_hit:\n",
        "            rank = retrieved_ids.index(expected_id) + 1\n",
        "            mrr = 1 / rank\n",
        "        else:\n",
        "            mrr = 0\n",
        "        eval_results.append(mrr)\n",
        "\n",
        "    # Return the average MRR across all queries as the final evaluation metric\n",
        "    return np.average(eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsVeVX2FEgBc"
      },
      "outputs": [],
      "source": [
        "embeddings_model_spec = {\n",
        "}\n",
        "\n",
        "embeddings_model_spec['OAI-Large-256']={'model_name':'text-embedding-3-large','dimensions':256, 'batch_size':512}\n",
        "embeddings_model_spec['OAI-Large-3072']={'model_name':'text-embedding-3-large','dimensions':3072, 'batch_size':512}\n",
        "embeddings_model_spec['OAI-Small']={'model_name':'text-embedding-3-small','dimensions':1536, 'batch_size':512}\n",
        "embeddings_model_spec['OAI-ada-002']={'model_name':'text-embedding-ada-002','dimensions':None, 'batch_size':512}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fYwLk9bEgBc"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "results = []\n",
        "\n",
        "languages = [\"FR\", \"EN\"]\n",
        "languages = [\"EN\"]\n",
        "\n",
        "# Loop through all languages\n",
        "for language in languages:\n",
        "\n",
        "    # Load dataset\n",
        "    file_name=language+\"_dataset.json\"\n",
        "    qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)\n",
        "\n",
        "    # Loop through all models\n",
        "    for model_name, model_spec in embeddings_model_spec.items():\n",
        "\n",
        "        print(\"Processing model : \"+str(model_spec))\n",
        "\n",
        "        # Get model\n",
        "        embed_model = OpenAIEmbedding(model=model_spec['model_name'],\n",
        "                                      embed_batch_size=model_spec['batch_size'],\n",
        "                                      dimensions=model_spec['dimensions']\n",
        "                                     )\n",
        "\n",
        "        start_time_assessment=time.time()\n",
        "\n",
        "        # Assess embedding score (in terms of MRR)\n",
        "        score = evaluate(qa_dataset, embed_model)\n",
        "\n",
        "        # Get duration of score assessment\n",
        "        duration_assessment = time.time()-start_time_assessment\n",
        "\n",
        "        results.append([language, model_name, score, duration_assessment])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KywoJ7JsFeoW"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results, columns = [\"Language\" ,\"Embedding model\", \"MRR\", \"Duration\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUHBnd8meXwS"
      },
      "outputs": [],
      "source": [
        "df.to_pickle(\"results_openai.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJYH-4q_eXwS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40meh1xEmNTW"
      },
      "source": [
        "## Plotting\n",
        "\n",
        "Let us plot the results using the Seaborn library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiQXqk2GeXwU"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a grouped bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=\"Embedding model\", y=\"MRR\", hue=\"Language\", data=df, palette=\"viridis\")\n",
        "\n",
        "plt.xlabel('Embedding Model')\n",
        "plt.ylabel('MRR  (Mean Reciprocal Rank)')\n",
        "plt.title('MRR by Model and Language')\n",
        "plt.xticks(rotation=45, ha=\"right\")  # Rotate model names for better readability\n",
        "plt.ylim(0, 0.8)\n",
        "\n",
        "plt.legend(title='Language')\n",
        "plt.tight_layout()  # Adjust layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbGB4BmsEgBe"
      },
      "source": [
        "# Evaluate open-source models\n",
        "\n",
        "## Install and import libraries\n",
        "\n",
        "Let us additionally install\n",
        "\n",
        "- sentence-transformers : For using the util.semantic_search function\n",
        "- bitsandbytes : For loading quantized models\n",
        "- accelerate : For more efficient model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioCMVMiRvnsK",
        "outputId": "2305b669-f489-4bc5-a9bb-a579f6f5c453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -q sentence-transformers bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kx6R8jLvnsK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAbUncdmshe"
      },
      "source": [
        "## Pooling functions\n",
        "\n",
        "Two main types of pooling functions: 'mean' and 'cls' (see https://blog.ml6.eu/the-art-of-pooling-embeddings-c56575114cf8).\n",
        "\n",
        "E5_Mistral additionally relies on 'last_token' pooling, see model card: https://huggingface.co/intfloat/e5-mistral-7b-instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxl6Kg81vnsK"
      },
      "outputs": [],
      "source": [
        "def mean_pooling(model_output):\n",
        "    return torch.mean(model_output[\"last_hidden_state\"], dim=1)\n",
        "\n",
        "def cls_pooling(model_output):\n",
        "    return model_output[0][:, 0]\n",
        "\n",
        "def last_token_pooling(model_output):\n",
        "    return model_output[0][:, -1]\n",
        "\n",
        "def get_sentence_embedding(text, tokenizer, embed_model, normalize, max_length, pooling_type='cls'):\n",
        "\n",
        "    if pooling_type==\"last_token\":\n",
        "        encoded_input = tokenizer(text, max_length=max_length, return_attention_mask=False, padding=False, truncation=True)\n",
        "        encoded_input['input_ids'] = encoded_input['input_ids'] + [tokenizer.eos_token_id]\n",
        "        encoded_input = tokenizer.pad([encoded_input], padding=True, return_attention_mask=True, return_tensors='pt').to(\"cuda\")\n",
        "    else:\n",
        "        encoded_input = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_output = embed_model(**encoded_input)\n",
        "\n",
        "    if pooling_type==\"cls\":\n",
        "        sentence_embeddings = cls_pooling(model_output)\n",
        "    if pooling_type==\"mean\":\n",
        "        sentence_embeddings = mean_pooling(model_output)\n",
        "    if pooling_type==\"last_token\":\n",
        "        sentence_embeddings = last_token_pooling(model_output)\n",
        "\n",
        "    if normalize:\n",
        "        sentence_embeddings = F.normalize(sentence_embeddings)\n",
        "\n",
        "    return sentence_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L-qJ4MOnQgV"
      },
      "source": [
        "## Evaluation function\n",
        "\n",
        "We slightly modify the evaluation function to avoid using the Llama Index wrapper for embedding, which does not support 'last_token' embedding. Top k results are computed using the util.semantic_search function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7b5v4r6vnsL"
      },
      "outputs": [],
      "source": [
        "def evaluate(qa_dataset, tokenizer, embed_model, normalize, max_length=None, pooling_type=\"cls\", top_k=5, verbose=False):\n",
        "\n",
        "    # Get corpus and queries, and relevant documents from the qa_dataset object\n",
        "    input_texts = list(qa_dataset.corpus.values())\n",
        "    input_text_keys = list(qa_dataset.corpus.keys())\n",
        "    queries = qa_dataset.queries\n",
        "    relevant_docs = qa_dataset.relevant_docs\n",
        "\n",
        "    # Compute embeddings for each document in the corpus\n",
        "    embeddings = [get_sentence_embedding(sentence, tokenizer, embed_model, normalize, max_length, pooling_type)\n",
        "                  for sentence in input_texts]\n",
        "    embeddings = torch.cat(embeddings)\n",
        "\n",
        "    # Prepare to collect evaluation results\n",
        "    eval_results = []\n",
        "\n",
        "    # Iterate over each query in the dataset to evaluate retrieval performance\n",
        "    for query_id, query in tqdm(queries.items()):\n",
        "\n",
        "        # Retrieve the top_k most similar documents for the current query and extract the IDs of the retrieved documents\n",
        "        query_embedding = get_sentence_embedding(query, tokenizer, embed_model, normalize, max_length, pooling_type)\n",
        "        results = util.semantic_search(query_embedding, embeddings, top_k=top_k)[0]\n",
        "        retrieved_ids = [input_text_keys[int(result[\"corpus_id\"])] for result in results]\n",
        "\n",
        "        # Check if the expected document was among the retrieved documents\n",
        "        expected_id = relevant_docs[query_id][0]\n",
        "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
        "\n",
        "        # Calculate the Mean Reciprocal Rank (MRR) and append to results\n",
        "        if is_hit:\n",
        "            rank = retrieved_ids.index(expected_id) + 1\n",
        "            mrr = 1 / rank\n",
        "        else:\n",
        "            mrr = 0\n",
        "        eval_results.append(mrr)\n",
        "\n",
        "    # Return the average MRR across all queries as the final evaluation metric\n",
        "    return np.average(eval_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXCP3VZboXOk"
      },
      "source": [
        "The code follows the same logic as for OpenAI embeddings: loop through all models and languages and compute MRR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q2sTsTiFeoa"
      },
      "outputs": [],
      "source": [
        "embeddings_model_spec = {\n",
        "}\n",
        "\n",
        "embeddings_model_spec['E5-mistral-7b']={'model_name':'intfloat/e5-mistral-7b-instruct','max_length':32768, 'pooling_type':'last_token',\n",
        "                                        'normalize': True, 'batch_size':1, 'kwargs': {'load_in_4bit':True, 'bnb_4bit_compute_dtype':torch.float16}}\n",
        "embeddings_model_spec['ML-E5-large']={'model_name':'intfloat/multilingual-e5-large','max_length':512, 'pooling_type':'mean',\n",
        "                                      'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
        "embeddings_model_spec['BGE-M3']={'model_name':'BAAI/bge-m3','max_length':8192, 'pooling_type':'cls',\n",
        "                                 'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
        "embeddings_model_spec['Nomic-Embed']={'model_name':'nomic-ai/nomic-embed-text-v1','max_length':8192, 'pooling_type':'mean',\n",
        "                                      'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'trust_remote_code' : True}}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBPlT8l-vnsN"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
        "\n",
        "results = []\n",
        "\n",
        "languages = [\"EN\", \"FR\", \"CS\", \"HU\"]\n",
        "\n",
        "# Loop through all models\n",
        "for model_name, model_spec in embeddings_model_spec.items():\n",
        "\n",
        "    print(\"Processing model : \"+str(model_spec))\n",
        "\n",
        "    # Get model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_spec['model_name'])\n",
        "    embed_model = AutoModel.from_pretrained(model_spec['model_name'], **model_spec['kwargs'])\n",
        "\n",
        "    if model_name==\"Nomic-Embed\":\n",
        "        embed_model.to('cuda')\n",
        "\n",
        "    # Loop through all languages\n",
        "    for language in languages:\n",
        "\n",
        "        # Load dataset\n",
        "        file_name=language+\"_dataset.json\"\n",
        "        qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)\n",
        "\n",
        "        start_time_assessment=time.time()\n",
        "\n",
        "        # Assess embedding score (in terms of hit rate at k=5)\n",
        "        score = evaluate(qa_dataset, tokenizer, embed_model, model_spec['normalize'], model_spec['max_length'], model_spec['pooling_type'])\n",
        "\n",
        "        # Get duration of score assessment\n",
        "        duration_assessment = time.time()-start_time_assessment\n",
        "\n",
        "        results.append([language, model_name, score, duration_assessment])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuJXB922Feob"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results, columns = [\"Language\" ,\"Embedding model\", \"MRR\", \"Duration\"])\n",
        "df.to_pickle(\"results_open_source.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV-d7zsYokyA"
      },
      "source": [
        "## Plotting\n",
        "\n",
        "Let us plot the results using the Seaborn library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ero6GyG7gF7E"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a grouped bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=\"Embedding model\", y=\"MRR\", hue=\"Language\", data=df, palette=\"viridis\")\n",
        "\n",
        "plt.xlabel('')\n",
        "plt.ylabel('MRR  (Mean Reciprocal Rank)')\n",
        "plt.title('MRR by Model and Language')\n",
        "plt.xticks(rotation=45, ha=\"right\")  # Rotate model names for better readability\n",
        "plt.ylim(0, 0.8)\n",
        "\n",
        "plt.legend(title='Language')\n",
        "plt.tight_layout()  # Adjust layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkQWLcp7Feob"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCYXHOZ1om2l"
      },
      "source": [
        "Let us plot the processing time per model for English language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxsFcWQ_Feob"
      },
      "outputs": [],
      "source": [
        "df_english=df[df['Language']==\"EN\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abSX1mMvFeoc"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df_english[\"Embedding model\"], df_english[\"Duration\"], color='blue', alpha=0.5)\n",
        "plt.title('Processing time per embedding model for English Language')\n",
        "#plt.xlabel('Embedding Model')\n",
        "plt.ylabel('Processing time (s)')\n",
        "plt.xticks(rotation=45, ha=\"right\")  # Rotate model names for better readability\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()  # Adjust layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbPVuNMviwJc"
      },
      "source": [
        "# Combine all MRR\n",
        "\n",
        "Let us finally combine the mean reciprocal ranks of all models/languages in a single figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC1igFBeiyKI"
      },
      "outputs": [],
      "source": [
        "df_open_source = pd.read_pickle(\"results_open_source.pkl\")\n",
        "df_openai = pd.read_pickle(\"results_openai.pkl\")\n",
        "df = pd.concat([df_openai, df_open_source])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UWCb3OuiyQR"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a grouped bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=\"Embedding model\", y=\"MRR\", hue=\"Language\", data=df, palette=\"viridis\")\n",
        "\n",
        "plt.xlabel('')\n",
        "plt.ylabel('MRR  (Mean Reciprocal Rank)')\n",
        "plt.title('MRR by Model and Language')\n",
        "plt.xticks(rotation=45, ha=\"right\")  # Rotate model names for better readability\n",
        "plt.ylim(0, 0.8)\n",
        "\n",
        "plt.legend(title='Language')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()  # Adjust layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QhLmo_JFeoc"
      },
      "source": [
        "# Additional resources\n",
        "\n",
        "- Related Medium post:\n",
        "- [Finetune embedding models with LlamaIndex](https://docs.llamaindex.ai/en/latest/examples/finetuning/embeddings/finetune_embedding.html#)\n",
        "- https://medium.com/towards-data-science/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWogrJ8IFeoc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "024f3018c5b049138bda5ff9331ff9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1359923c9b314fca92cf6ec84edd44b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "519cd748fc7347b68c86dc9e6b00bc49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aea427f122c490990c6d3e86b971236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a1d6f9bc3ed44c58ba9871b679c570c",
            "placeholder": "​",
            "style": "IPY_MODEL_b592ecdb59094a0e998a76e302047770",
            "value": "Parsing nodes: 100%"
          }
        },
        "6dcfc94deb394da5af5a32870e5afd22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79e705f2caf540a586a4e41e7f091656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dcfc94deb394da5af5a32870e5afd22",
            "placeholder": "​",
            "style": "IPY_MODEL_024f3018c5b049138bda5ff9331ff9fd",
            "value": " 1/1 [00:00&lt;00:00,  1.65it/s]"
          }
        },
        "8a1d6f9bc3ed44c58ba9871b679c570c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97ce0a57af614a47b11fbfb9965a1b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c4ce4fbf6db49ac8cd1c92dd691f18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5aea427f122c490990c6d3e86b971236",
              "IPY_MODEL_affb1620da1f4613a07541be3c58fc88",
              "IPY_MODEL_79e705f2caf540a586a4e41e7f091656"
            ],
            "layout": "IPY_MODEL_519cd748fc7347b68c86dc9e6b00bc49"
          }
        },
        "affb1620da1f4613a07541be3c58fc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1359923c9b314fca92cf6ec84edd44b5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97ce0a57af614a47b11fbfb9965a1b48",
            "value": 1
          }
        },
        "b592ecdb59094a0e998a76e302047770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}